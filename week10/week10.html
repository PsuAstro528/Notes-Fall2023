<!DOCTYPE html>
<HTML lang = "en">
<HEAD>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  
<style>
pre.hljl {
    border: 1px solid #ccc;
    margin: 5px;
    padding: 5px;
    overflow-x: auto;
    color: rgb(68,68,68); background-color: rgb(251,251,251); }
pre.hljl > span.hljl-t { }
pre.hljl > span.hljl-w { }
pre.hljl > span.hljl-e { }
pre.hljl > span.hljl-eB { }
pre.hljl > span.hljl-o { }
pre.hljl > span.hljl-k { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kc { color: rgb(59,151,46); font-style: italic; }
pre.hljl > span.hljl-kd { color: rgb(214,102,97); font-style: italic; }
pre.hljl > span.hljl-kn { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kp { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kr { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kt { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-n { }
pre.hljl > span.hljl-na { }
pre.hljl > span.hljl-nb { }
pre.hljl > span.hljl-nbp { }
pre.hljl > span.hljl-nc { }
pre.hljl > span.hljl-ncB { }
pre.hljl > span.hljl-nd { color: rgb(214,102,97); }
pre.hljl > span.hljl-ne { }
pre.hljl > span.hljl-neB { }
pre.hljl > span.hljl-nf { color: rgb(66,102,213); }
pre.hljl > span.hljl-nfm { color: rgb(66,102,213); }
pre.hljl > span.hljl-np { }
pre.hljl > span.hljl-nl { }
pre.hljl > span.hljl-nn { }
pre.hljl > span.hljl-no { }
pre.hljl > span.hljl-nt { }
pre.hljl > span.hljl-nv { }
pre.hljl > span.hljl-nvc { }
pre.hljl > span.hljl-nvg { }
pre.hljl > span.hljl-nvi { }
pre.hljl > span.hljl-nvm { }
pre.hljl > span.hljl-l { }
pre.hljl > span.hljl-ld { color: rgb(148,91,176); font-style: italic; }
pre.hljl > span.hljl-s { color: rgb(201,61,57); }
pre.hljl > span.hljl-sa { color: rgb(201,61,57); }
pre.hljl > span.hljl-sb { color: rgb(201,61,57); }
pre.hljl > span.hljl-sc { color: rgb(201,61,57); }
pre.hljl > span.hljl-sd { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdB { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdC { color: rgb(201,61,57); }
pre.hljl > span.hljl-se { color: rgb(59,151,46); }
pre.hljl > span.hljl-sh { color: rgb(201,61,57); }
pre.hljl > span.hljl-si { }
pre.hljl > span.hljl-so { color: rgb(201,61,57); }
pre.hljl > span.hljl-sr { color: rgb(201,61,57); }
pre.hljl > span.hljl-ss { color: rgb(201,61,57); }
pre.hljl > span.hljl-ssB { color: rgb(201,61,57); }
pre.hljl > span.hljl-nB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nbB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nfB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nh { color: rgb(59,151,46); }
pre.hljl > span.hljl-ni { color: rgb(59,151,46); }
pre.hljl > span.hljl-nil { color: rgb(59,151,46); }
pre.hljl > span.hljl-noB { color: rgb(59,151,46); }
pre.hljl > span.hljl-oB { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-ow { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-p { }
pre.hljl > span.hljl-c { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-ch { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cm { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cp { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cpB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cs { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-csB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-g { }
pre.hljl > span.hljl-gd { }
pre.hljl > span.hljl-ge { }
pre.hljl > span.hljl-geB { }
pre.hljl > span.hljl-gh { }
pre.hljl > span.hljl-gi { }
pre.hljl > span.hljl-go { }
pre.hljl > span.hljl-gp { }
pre.hljl > span.hljl-gs { }
pre.hljl > span.hljl-gsB { }
pre.hljl > span.hljl-gt { }
</style>



  <style type="text/css">
  @font-face {
  font-style: normal;
  font-weight: 300;
}
@font-face {
  font-style: normal;
  font-weight: 400;
}
@font-face {
  font-style: normal;
  font-weight: 600;
}
html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  font-family: monospace, monospace;
  font-size : 0.8em;
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
thead th {
    border-bottom: 1px solid black;
    background-color: white;
}
tr:nth-child(odd){
  background-color: rgb(248,248,248);
}


/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 3.6rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.4rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.2rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.8rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 2.4rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

p {
  margin-top: 0; }
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }
ul {
  list-style: circle; }
ol {
  list-style: decimal; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li > p {margin : 0;}
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 1.0rem; }
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }

pre {
  display: block;
  padding: 9.5px;
  margin: 0 0 10px;
  font-size: 13px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre.hljl {
  margin: 0 0 10px;
  display: block;
  background: #f5f5f5;
  border-radius: 4px;
  padding : 5px;
}

pre.output {
  background: #ffffff;
}

pre.code {
  background: #ffffff;
}

pre.julia-error {
  color : red
}

code,
kbd,
pre,
samp {
  font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
  font-size: 0.9em;
}


@media (min-width: 400px) {}
@media (min-width: 550px) {}
@media (min-width: 750px) {}
@media (min-width: 1000px) {}
@media (min-width: 1200px) {}

h1.title {margin-top : 20px}
img {max-width : 100%}
div.title {text-align: center;}

  </style>
</HEAD>

<BODY>
  <div class ="container">
    <div class = "row">
      <div class = "col-md-12 twelve columns">
        <div class="title">
          
          
          
        </div>

        <h1>Week 10 Discussion &amp; Q&amp;A:</h1>
<h3>Parallelization for Hardware Accelerators &#40;e.g., GPUs&#41;</h3>


<pre class='hljl'>
<span class='hljl-k'>import</span><span class='hljl-t'> </span><span class='hljl-n'>Pkg</span><span class='hljl-t'>
</span><span class='hljl-n'>Pkg</span><span class='hljl-oB'>.</span><span class='hljl-nf'>activate</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;.&quot;</span><span class='hljl-p'>)</span>
</pre>




<pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>Test</span>
</pre>




<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>versioninfo</span><span class='hljl-p'>()</span>
</pre>


<pre class="output">
CUDA runtime 12.2, artifact installation
CUDA driver 12.2
NVIDIA driver 535.104.12

CUDA libraries: 
- CUBLAS: 12.2.5
- CURAND: 10.3.3
- CUFFT: 11.0.8
- CUSOLVER: 11.5.2
- CUSPARSE: 12.1.2
- CUPTI: 20.0.0
- NVML: 12.0.0&#43;535.104.12

Julia packages: 
- CUDA: 5.0.0
- CUDA_Driver_jll: 0.6.0&#43;4
- CUDA_Runtime_jll: 0.9.2&#43;3

Toolchain:
- Julia: 1.9.2
- LLVM: 14.0.6
- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7
.0, 7.1, 7.2, 7.3, 7.4, 7.5
- Device capability support: sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_6
2, sm_70, sm_72, sm_75, sm_80, sm_86

1 device:
  0: Tesla P100-PCIE-12GB &#40;sm_60, 11.166 GiB / 12.000 GiB available&#41;
</pre>


<p><strong>Q:</strong> Are there any physical differences in the hardware between CPU cores and GPU cores &#40;besides the fact that they are smaller&#41;?</p>

<h1>P100 GPU layout</h1>
<p><img src="https://cdn.arstechnica.net/wp-content/uploads/sites/3/2016/04/gp100_block_diagram-1.png" alt="P100 Block diagram" />&quot;</p>

<h3>CPU chip layout</h3>
<p><img src="https://cdn.arstechnica.net/wp-content/uploads/2011/11/core_i7_lga_2011_die-4ec188e-intro.jpg" alt="Intel CPU die shot" /></p>


<pre class='hljl'>
<span class='hljl-p'>[</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>capability</span><span class='hljl-p'>(</span><span class='hljl-n'>dev</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>dev</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>devices</span><span class='hljl-p'>()]</span>
</pre>


<pre class="output">
1-element Vector&#123;VersionNumber&#125;:
 v&quot;6.0.0&quot;
</pre>


<h2>Calling GPU on arrays</h2>


<pre class='hljl'>
<span class='hljl-n'>N</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>10</span><span class='hljl-oB'>^</span><span class='hljl-ni'>6</span>
</pre>


<pre class="output">
1000000
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-n'>x_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>fill</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1.0f0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>N</span><span class='hljl-p'>)</span><span class='hljl-t'>  </span><span class='hljl-cs'># a vector stored on the GPU filled with 1.0 (Float32)</span><span class='hljl-t'>
	</span><span class='hljl-n'>y_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>fill</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0f0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>N</span><span class='hljl-p'>)</span><span class='hljl-t'>  </span><span class='hljl-cs'># a vector stored on the GPU filled with 2.0</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-p'>;</span><span class='hljl-t'>  </span><span class='hljl-cs'># Suppress output to notebook, unless enable scalar indexing</span>
</pre>




<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-n'>y_d</span><span class='hljl-t'> </span><span class='hljl-oB'>.+=</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@test</span><span class='hljl-t'> </span><span class='hljl-nf'>all</span><span class='hljl-p'>(</span><span class='hljl-nf'>Array</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.==</span><span class='hljl-t'> </span><span class='hljl-nfB'>3.0f0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
Test Passed
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>add_broadcast!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'># GPU kernels are run asynchronously.  </span><span class='hljl-t'>
    </span><span class='hljl-cs'># If you want to ensure that calculations are finished, you&#39;ll nee to use `CUDA.@sync`   </span><span class='hljl-t'>
    </span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>.+=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
add_broadcast&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>x_h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>fill</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1.0f0</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>y_h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>fill</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0f0</span><span class='hljl-p'>,</span><span class='hljl-n'>N</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-n'>y_h</span><span class='hljl-t'> </span><span class='hljl-oB'>.+=</span><span class='hljl-t'> </span><span class='hljl-n'>x_h</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-n'>y_h</span><span class='hljl-t'> </span><span class='hljl-oB'>.+=</span><span class='hljl-t'> </span><span class='hljl-n'>x_h</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-n'>y_h</span><span class='hljl-t'> </span><span class='hljl-oB'>.+=</span><span class='hljl-t'> </span><span class='hljl-n'>x_h</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-n'>y_h</span><span class='hljl-t'> </span><span class='hljl-oB'>.+=</span><span class='hljl-t'> </span><span class='hljl-n'>x_h</span><span class='hljl-p'>;</span>
</pre>


<pre class="output">
0.083283 seconds &#40;90.27 k allocations: 6.043 MiB, 22.83&#37; gc time&#41;
  0.000655 seconds &#40;2 allocations: 64 bytes&#41;
  0.000738 seconds &#40;2 allocations: 64 bytes&#41;
  0.000559 seconds &#40;2 allocations: 64 bytes&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>x_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>add_broadcast!</span><span class='hljl-t'>
</span><span class='hljl-n'>z_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>fill</span><span class='hljl-p'>(</span><span class='hljl-nfB'>2.0f0</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>N</span><span class='hljl-p'>)</span><span class='hljl-t'>  
</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-nf'>add_broadcast!</span><span class='hljl-p'>(</span><span class='hljl-n'>z_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-nf'>add_broadcast!</span><span class='hljl-p'>(</span><span class='hljl-n'>z_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nf'>add_broadcast!</span><span class='hljl-p'>(</span><span class='hljl-n'>z_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@time</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nf'>add_broadcast!</span><span class='hljl-p'>(</span><span class='hljl-n'>z_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
0.022327 seconds &#40;10.07 k allocations: 684.856 KiB&#41;
  0.000031 seconds &#40;31 allocations: 1.859 KiB&#41;
  0.000069 seconds &#40;31 allocations: 1.859 KiB&#41;
  0.000073 seconds &#40;31 allocations: 1.859 KiB&#41;
</pre>


<p><strong>Q:</strong> Would it make any sense to use a GPU over a CPU in parallel to run a for loop or something of that nature?</p>

<h2>How to use GPUs very ineefficiently</h2>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add1!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>y</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'> </span><span class='hljl-n'>nothing</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_add1&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu1!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-k'>begin</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add1!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
bench_gpu1&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-nf'>bench_gpu1!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu1!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
0.178834656
</pre>


<h2>Parallelize over multiple threads</h2>
<p><strong>Q:</strong> Can you run &#39;for&#39; loops in parallel on a GPU?</p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>    </span><span class='hljl-cs'># this example only requires linear indexing, so just use `x`</span><span class='hljl-t'>
    </span><span class='hljl-n'>stride</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>blockDim</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-oB'>:</span><span class='hljl-n'>stride</span><span class='hljl-oB'>:</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>y</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'> </span><span class='hljl-n'>nothing</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_add2&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>let</span><span class='hljl-t'>
	</span><span class='hljl-nf'>fill!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@test</span><span class='hljl-t'> </span><span class='hljl-nf'>all</span><span class='hljl-p'>(</span><span class='hljl-nf'>Array</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.==</span><span class='hljl-t'> </span><span class='hljl-nfB'>3.0f0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
Test Passed
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-k'>begin</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
bench_gpu2&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-nf'>bench_gpu2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
0.002022622
</pre>


<p><strong>Q:</strong> Can you explain what does it mean that &quot;all calls to the GPU are scheduled asynchronous&quot;?</p>
<p><strong>A:</strong> You have to explicitly state when you want a sychronization event.</p>


<pre class='hljl'>
<span class='hljl-nd'>@time</span><span class='hljl-t'>  </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'> 
</span><span class='hljl-nd'>@time</span><span class='hljl-t'>  </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
0.000043 seconds &#40;27 allocations: 1.375 KiB&#41;
  0.000071 seconds &#40;27 allocations: 1.375 KiB&#41;
CUDA.HostKernel for gpu_add2&#33;&#40;CUDA.CuDeviceVector&#123;Float32, 1&#125;, CUDA.CuDevic
eVector&#123;Float32, 1&#125;&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@time</span><span class='hljl-t'>  </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'> 
</span><span class='hljl-nd'>@time</span><span class='hljl-t'>  </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add2!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
0.002077 seconds &#40;30 allocations: 1.453 KiB&#41;
  0.002153 seconds &#40;30 allocations: 1.453 KiB&#41;
CUDA.HostKernel for gpu_add2&#33;&#40;CUDA.CuDeviceVector&#123;Float32, 1&#125;, CUDA.CuDevic
eVector&#123;Float32, 1&#125;&#41;
</pre>


<h2>Paralelizing with multiple blocks</h2>
<p><img src="https://cuda.juliagpu.org/dev/tutorials/intro1.png" alt="CUDA array indexing" /></p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-nf'>blockDim</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-k'>if</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>)</span><span class='hljl-t'>
		</span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>y</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'>
	</span><span class='hljl-k'>end</span><span class='hljl-t'>
	</span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_add3&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>let</span><span class='hljl-t'>
	</span><span class='hljl-n'>numblocks</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ceil</span><span class='hljl-p'>(</span><span class='hljl-n'>Int</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>N</span><span class='hljl-oB'>/</span><span class='hljl-ni'>256</span><span class='hljl-p'>)</span><span class='hljl-t'>
	
	</span><span class='hljl-nf'>fill!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>numblocks</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@test</span><span class='hljl-t'> </span><span class='hljl-nf'>all</span><span class='hljl-p'>(</span><span class='hljl-nf'>Array</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.==</span><span class='hljl-t'> </span><span class='hljl-nfB'>3.0f0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
Test Passed
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>numblocks</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ceil</span><span class='hljl-p'>(</span><span class='hljl-n'>Int</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-ni'>256</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-k'>begin</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-ni'>256</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>numblocks</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
bench_gpu3&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-nf'>bench_gpu3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
5.3245e-5
</pre>


<h3>Choosing Number of threads/block &amp; Number of blocks</h3>


<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
56
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
1024
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-ni'>1024</span><span class='hljl-t'> </span><span class='hljl-cs'>#KB</span>
</pre>


<pre class="output">
48.0
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_WARP_SIZE</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
32
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X</span><span class='hljl-p'>),</span><span class='hljl-t'> 
</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y</span><span class='hljl-p'>),</span><span class='hljl-t'>
</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;1024, 1024, 64&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_GRID_DIM_X</span><span class='hljl-p'>),</span><span class='hljl-t'> 
</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#40;2147483647, 65535, 65535&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>attribute</span><span class='hljl-p'>(</span><span class='hljl-nf'>device</span><span class='hljl-p'>(),</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-n'>DEVICE_ATTRIBUTE_L2_CACHE_SIZE</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-ni'>1024</span><span class='hljl-oB'>^</span><span class='hljl-ni'>2</span><span class='hljl-t'> </span><span class='hljl-cs'># MB</span>
</pre>


<pre class="output">
3.0
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-n'>kernel</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>launch</span><span class='hljl-oB'>=</span><span class='hljl-kc'>false</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>config</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>launch_configuration</span><span class='hljl-p'>(</span><span class='hljl-n'>kernel</span><span class='hljl-oB'>.</span><span class='hljl-n'>fun</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>threads3</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>min</span><span class='hljl-p'>(</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>config</span><span class='hljl-oB'>.</span><span class='hljl-n'>threads</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>blocks3</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cld</span><span class='hljl-p'>(</span><span class='hljl-n'>N</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>threads3</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>threads3</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>blocks3</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
&#40;1024, 977&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-nf'>fill!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nf'>kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-n'>threads3</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>blocks3</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@test</span><span class='hljl-t'> </span><span class='hljl-nf'>all</span><span class='hljl-p'>(</span><span class='hljl-nf'>Array</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.==</span><span class='hljl-t'> </span><span class='hljl-nfB'>3.0f0</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
Test Passed
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu4!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>kernel</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>launch</span><span class='hljl-oB'>=</span><span class='hljl-kc'>false</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_add3!</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>config</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>launch_configuration</span><span class='hljl-p'>(</span><span class='hljl-n'>kernel</span><span class='hljl-oB'>.</span><span class='hljl-n'>fun</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>threads4</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>min</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>config</span><span class='hljl-oB'>.</span><span class='hljl-n'>threads</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>blocks4</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cld</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>threads4</span><span class='hljl-p'>)</span><span class='hljl-t'>

    </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-k'>begin</span><span class='hljl-t'>
        </span><span class='hljl-nf'>kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-n'>threads4</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>blocks4</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
bench_gpu4&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-nf'>bench_gpu4!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu4!</span><span class='hljl-p'>(</span><span class='hljl-n'>y_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>x_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
5.2189e-5
</pre>


<p><strong>Q:</strong> I am still unsure about the implementation of a GPU kernal. How common is this and how should we know whether to use it over a generic GPU array interface?</p>
<p><strong>A:</strong> Generaly, I&#39;d try the array interface first &#40;if you can&#41;.  Then decide if you want to try to get further performance boost by reducing memory transfers.</p>

<h3>Generating Pseudo-random numbers on the GPU</h3>


<pre class='hljl'>
<span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-n'>rand_nums_h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-n'>N</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
0.13587316
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-n'>rand_nums_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-n'>N</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
0.569196675
</pre>



<pre class='hljl'>
<span class='hljl-n'>rand_nums_h</span>
</pre>


<pre class="output">
1000000-element Vector&#123;Float64&#125;:
 0.6879696280197086
 0.5032090919247147
 0.7791856068030752
 0.29355437146459895
 0.4405612594806362
 0.06755844243213227
 0.4381513039636865
 0.6834742367233528
 0.8414358865611535
 0.5510702270846274
 0.5156433949058279
 0.34725541275838345
 0.8621208321294287
 0.5442895751196315
 ⋮
 0.9138208287927976
 0.8499825272572508
 0.4360981392543877
 0.6252207119597106
 0.3347409995609587
 0.5348679254280373
 0.25678986764622225
 0.18075486128794116
 0.4237586741614854
 0.2863812774327106
 0.186074919449422
 0.4076048859700201
 0.7429136956647424
</pre>



<pre class='hljl'>
<span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>rand_nums_d</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
1000000-element Vector&#123;Float32&#125;:
 0.9847321
 0.7582767
 0.2747971
 0.06025254
 0.18360691
 0.93456244
 0.57306564
 0.13025342
 0.58057606
 0.77656764
 0.10709424
 0.88696307
 0.32356238
 0.87416947
 ⋮
 0.082201414
 0.6620423
 0.22277841
 0.63350964
 0.46225932
 0.7771866
 0.46655184
 0.46557704
 0.46574757
 0.25220126
 0.84250367
 0.88466334
 0.481679
</pre>



<pre class='hljl'>
<span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@allowscalar</span><span class='hljl-t'> </span><span class='hljl-n'>rand_nums_d</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span>
</pre>


<pre class="output">
0.9847321f0
</pre>


<h2>Using GPU for a more substantial function</h2>


<pre class='hljl'>
<span class='hljl-s'>&quot;&quot;&quot;
   ecc_anom_init_guess_danby(mean_anomaly, eccentricity)
Returns initial guess for the eccentric anomaly for use by itterative solvers of Kepler&#39;s equation for bound orbits.  
Based on &quot;The Solution of Kepler&#39;s Equations - Part Three&quot;
Danby, J. M. A. (1987) Journal: Celestial Mechanics, Volume 40, Issue 3-4, pp. 303-312 (1987CeMec..40..303D)
&quot;&quot;&quot;</span><span class='hljl-t'>
</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>ecc_anom_init_guess_danby</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>my_pi</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>),</span><span class='hljl-n'>pi</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-n'>my_pi</span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-n'>my_pi</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-ni'>0</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-t'>
    </span><span class='hljl-n'>k</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nfB'>0.85</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>if</span><span class='hljl-t'>  </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;</span><span class='hljl-t'> </span><span class='hljl-nf'>zero</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>)</span><span class='hljl-t'>
		</span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>+=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-n'>my_pi</span><span class='hljl-t'>
	</span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>E</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-oB'>&lt;</span><span class='hljl-n'>my_pi</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-n'>k</span><span class='hljl-oB'>*</span><span class='hljl-n'>ecc</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>k</span><span class='hljl-oB'>*</span><span class='hljl-n'>ecc</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-p'>;</span>
</pre>




<pre class='hljl'>
<span class='hljl-s'>&quot;&quot;&quot;
   update_ecc_anom_laguerre(eccentric_anomaly_guess, mean_anomaly, eccentricity)
Update the current guess for solution to Kepler&#39;s equation
  
Based on &quot;An Improved Algorithm due to Laguerre for the Solution of Kepler&#39;s Equation&quot;
   Conway, B. A.  (1986) Celestial Mechanics, Volume 39, Issue 2, pp.199-211 (1986CeMec..39..199C)
&quot;&quot;&quot;</span><span class='hljl-t'>
</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>update_ecc_anom_laguerre</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-n'>es</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-oB'>*</span><span class='hljl-nf'>sin</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-n'>ec</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-oB'>*</span><span class='hljl-nf'>cos</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>)</span><span class='hljl-t'>
  </span><span class='hljl-cs'>#(es, ec) = ecc .* sincos(E)  # Does combining them provide any speed benefit?</span><span class='hljl-t'>
  </span><span class='hljl-n'>F</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-oB'>-</span><span class='hljl-n'>es</span><span class='hljl-p'>)</span><span class='hljl-oB'>-</span><span class='hljl-n'>M</span><span class='hljl-t'>
  </span><span class='hljl-n'>Fp</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>one</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>)</span><span class='hljl-oB'>-</span><span class='hljl-n'>ec</span><span class='hljl-t'>
  </span><span class='hljl-n'>Fpp</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>es</span><span class='hljl-t'>
  </span><span class='hljl-n'>n</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>5</span><span class='hljl-t'>
  </span><span class='hljl-n'>root</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>(</span><span class='hljl-nf'>abs</span><span class='hljl-p'>((</span><span class='hljl-n'>n</span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-p'>((</span><span class='hljl-n'>n</span><span class='hljl-oB'>-</span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-n'>Fp</span><span class='hljl-oB'>*</span><span class='hljl-n'>Fp</span><span class='hljl-oB'>-</span><span class='hljl-n'>n</span><span class='hljl-oB'>*</span><span class='hljl-n'>F</span><span class='hljl-oB'>*</span><span class='hljl-n'>Fpp</span><span class='hljl-p'>)))</span><span class='hljl-t'>
  </span><span class='hljl-n'>denom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>Fp</span><span class='hljl-oB'>&gt;</span><span class='hljl-nf'>zero</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>?</span><span class='hljl-t'> </span><span class='hljl-n'>Fp</span><span class='hljl-oB'>+</span><span class='hljl-n'>root</span><span class='hljl-t'> </span><span class='hljl-oB'>:</span><span class='hljl-t'> </span><span class='hljl-n'>Fp</span><span class='hljl-oB'>-</span><span class='hljl-n'>root</span><span class='hljl-t'>
  </span><span class='hljl-k'>return</span><span class='hljl-t'> </span><span class='hljl-n'>E</span><span class='hljl-oB'>-</span><span class='hljl-n'>n</span><span class='hljl-oB'>*</span><span class='hljl-n'>F</span><span class='hljl-oB'>/</span><span class='hljl-n'>denom</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-p'>;</span>
</pre>




<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_ecc_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0e-8</span><span class='hljl-p'>)</span><span class='hljl-t'>
  	</span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-ni'>0</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nfB'>1.0</span><span class='hljl-t'>
	</span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nfB'>1e-16</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-t'>
  	</span><span class='hljl-cs'>#M = rem2pi(mean_anom,RoundNearest)</span><span class='hljl-t'>
    </span><span class='hljl-n'>M</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>mod</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-nf'>convert</span><span class='hljl-p'>(</span><span class='hljl-nf'>typeof</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>),</span><span class='hljl-n'>pi</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-n'>E</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>ecc_anom_init_guess_danby</span><span class='hljl-p'>(</span><span class='hljl-n'>M</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-kd'>local</span><span class='hljl-t'> </span><span class='hljl-n'>E_old</span><span class='hljl-t'>
    </span><span class='hljl-n'>max_its_laguerre</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>200</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-n'>max_its_laguerre</span><span class='hljl-t'>
       </span><span class='hljl-n'>E_old</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>E</span><span class='hljl-t'>
       </span><span class='hljl-n'>E</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>update_ecc_anom_laguerre</span><span class='hljl-p'>(</span><span class='hljl-n'>E_old</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>M</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'>
       </span><span class='hljl-k'>if</span><span class='hljl-t'> </span><span class='hljl-nf'>abs</span><span class='hljl-p'>(</span><span class='hljl-n'>E</span><span class='hljl-oB'>-</span><span class='hljl-n'>E_old</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-t'> </span><span class='hljl-k'>break</span><span class='hljl-t'> </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'> </span><span class='hljl-n'>E</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_ecc_anom &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_true_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>e</span><span class='hljl-oB'>::</span><span class='hljl-n'>Real</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>true_anom</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-oB'>*</span><span class='hljl-nf'>atan</span><span class='hljl-p'>(</span><span class='hljl-nf'>sqrt</span><span class='hljl-p'>((</span><span class='hljl-ni'>1</span><span class='hljl-oB'>+</span><span class='hljl-n'>e</span><span class='hljl-p'>)</span><span class='hljl-oB'>/</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>-</span><span class='hljl-n'>e</span><span class='hljl-p'>))</span><span class='hljl-oB'>*</span><span class='hljl-nf'>tan</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-oB'>/</span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
calc_true_anom &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_ecc_anom!</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-cs'>#@assert 1e-16 &lt;= tol &lt; 1</span><span class='hljl-t'>
    </span><span class='hljl-cs'>#@assert size(ecc_anom) == size(mean_anom) == size(ecc) </span><span class='hljl-t'>
	</span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>*</span><span class='hljl-t'> </span><span class='hljl-nf'>blockDim</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>
	</span><span class='hljl-k'>if</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>)</span><span class='hljl-t'>
		</span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>calc_ecc_anom</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>],</span><span class='hljl-n'>ecc</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>])</span><span class='hljl-t'>
	</span><span class='hljl-k'>end</span><span class='hljl-t'>
	</span><span class='hljl-k'>return</span><span class='hljl-t'> 
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_calc_ecc_anom&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu_calc_ecc_anom!</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@assert</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>==</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc</span><span class='hljl-p'>)</span><span class='hljl-t'> 
	</span><span class='hljl-n'>kernel</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>launch</span><span class='hljl-oB'>=</span><span class='hljl-kc'>false</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_ecc_anom!</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>config</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>launch_configuration</span><span class='hljl-p'>(</span><span class='hljl-n'>kernel</span><span class='hljl-oB'>.</span><span class='hljl-n'>fun</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>min</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>config</span><span class='hljl-oB'>.</span><span class='hljl-n'>threads</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>cld</span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-p'>)</span><span class='hljl-t'>

    </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-k'>begin</span><span class='hljl-t'>
        </span><span class='hljl-nf'>kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>mean_anom</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>ecc</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>tol</span><span class='hljl-p'>;</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-n'>threads</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>blocks</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
bench_gpu_calc_ecc_anom&#33; &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>M</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1000000</span>
</pre>


<pre class="output">
1000000
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-n'>ecc_anom_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>zeros</span><span class='hljl-p'>(</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-n'>M</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>ecc_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-n'>M</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>mean_anom_d</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-n'>Float64</span><span class='hljl-p'>,</span><span class='hljl-n'>M</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-n'>mean_anom_d</span><span class='hljl-t'> </span><span class='hljl-oB'>.*=</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-n'>π</span><span class='hljl-t'>
	</span><span class='hljl-n'>tol</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nfB'>1e-8</span><span class='hljl-t'>
	</span><span class='hljl-n'>kepler_kernel</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>launch</span><span class='hljl-oB'>=</span><span class='hljl-kc'>false</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_calc_ecc_anom!</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom_d</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>mean_anom_d</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc_d</span><span class='hljl-p'>,</span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>kepler_config</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>launch_configuration</span><span class='hljl-p'>(</span><span class='hljl-n'>kepler_kernel</span><span class='hljl-oB'>.</span><span class='hljl-n'>fun</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
&#40;blocks &#61; 112, threads &#61; 576&#41;
</pre>



<pre class='hljl'>
<span class='hljl-ni'>1000</span><span class='hljl-oB'>*</span><span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-nf'>bench_gpu_calc_ecc_anom!</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom_d</span><span class='hljl-p'>,</span><span class='hljl-n'>mean_anom_d</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc_d</span><span class='hljl-p'>,</span><span class='hljl-n'>tol</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
83.743085
</pre>



<pre class='hljl'>
<span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom_d</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
1000000-element Vector&#123;Float64&#125;:
 4.588595649753934
 3.657417760865393
 5.113773395865323
 5.232477710011794
 3.632795060761569
 2.34880398700522
 1.9945722960199732
 4.85174201306915
 4.427866509409915
 3.0623953811055826
 4.1690878312116535
 2.137072258304912
 2.8184246764919223
 1.624170521995028
 ⋮
 1.0692723748018478
 3.8965601853719165
 3.762751547317824
 1.3302007249922112
 4.89699201178899
 1.1366375439130019
 3.7805938457515285
 3.8783886904439546
 3.453481754751317
 3.0397477205928842
 0.744534710140274
 4.554892991725758
 6.057370356069308
</pre>



<pre class='hljl'>
<span class='hljl-k'>begin</span><span class='hljl-t'>
	</span><span class='hljl-n'>ecc_anom_h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>mean_anom_h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	</span><span class='hljl-n'>ecc_h</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_d</span><span class='hljl-p'>)</span><span class='hljl-t'>
	
	</span><span class='hljl-ni'>1000</span><span class='hljl-oB'>*</span><span class='hljl-nd'>@elapsed</span><span class='hljl-t'> </span><span class='hljl-n'>ecc_anom_h_comp</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>calc_ecc_anom</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>mean_anom_h</span><span class='hljl-p'>,</span><span class='hljl-n'>ecc_h</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
278.955075
</pre>



<pre class='hljl'>
<span class='hljl-nf'>maximum</span><span class='hljl-p'>(</span><span class='hljl-n'>abs</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-n'>ecc_anom_h_comp</span><span class='hljl-t'> </span><span class='hljl-oB'>.-</span><span class='hljl-t'> </span><span class='hljl-n'>ecc_anom_h</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
2.842170943040401e-14
</pre>


<h2>GPU Reductions</h2>

<p><img src="gpu_reduction.png" alt="GPU Reduction" /></p>

<h3>Q&amp;A</h3>
<p><strong>Q:</strong> Is it always faster to use GPU than CPU? When is CPU a better option than GPU?</p>
<p><strong>A:</strong> No.  See Lab 8.</p>
<p><strong>Q:</strong> GPUs are designed for running parallel structure operations, so I don&#39;t understand when it is advantageous to run a single-stream CPU.</p>
<p><strong>A:</strong> When you don&#39;t have enough computing &#40;per memory access&#41; to justify using a GPU.</p>
<p><strong>Q:</strong> It seems like GPUs can be quite complicated to work with, and in some cases favor lower precision outputs for higher performance. In what circumstances would one choose to work on a GPU rather than CPUs?</p>
<p><strong>A:</strong> When there&#39;s a &gt;10x performance benefit &#40;for your required accuracy&#41;. </p>
<p><strong>Q:</strong> How can we determine whether a certain Julia type is compatible with a GPU?        </p>
<p><strong>A:</strong> Your standard integers, floats and complex types are implemented.  Arrays, and that&#39;s about it.</p>
<p><strong>Q:</strong> Why is there latency for RAM to communicate with the GPU&#39;s VRAM? Could a system be converted fully to VRAM or can VRAM not operate as RAM?    </p>
<p><strong>A:</strong> They&#39;re two separate memory systems.</p>
<p><strong>Q:</strong>  What is stopping/discouraging us from building computer systems primarily with GPUs instead of CPUs so as to alleviate the memory transfer limitation for GPUs? <strong>A:</strong>  Not all computations run efficiently on GPUs</p>


<pre class='hljl'>

</pre>




        <HR/>
        <div class="footer">
          <p>
            Published from <a href="week10.ipynb">week10.ipynb</a>
            using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.12 on 2023-10-23.
          </p>
        </div>
      </div>
    </div>
  </div>
</BODY>

</HTML>
